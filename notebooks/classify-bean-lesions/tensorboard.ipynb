{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch library\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "# optim is where we get the stochastic gradient descent optimiser from\n",
    "import torch.optim as optim\n",
    "# transforms perform a set of operations to an individual item (image) so that it is usable by the model\n",
    "import torchvision.transforms as transforms\n",
    "# Tools for creating datasets from folders\n",
    "from torchvision import datasets\n",
    "# default_loader enables basic loading of images to be used in the CustomDataset class\n",
    "from torchvision.datasets.folder import default_loader\n",
    "# DataLoader creates batches of data in a format the model can use\n",
    "# Dataset is a general class that allows the retrieval of individual pieces of data via the DataLoader\n",
    "# All classes that inherit from Dataset must implement the __getitem__ and __len__ \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# We use the current time info for naming logs\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'tensorboard_initial'\n",
    "run_name = 'first_experiment'\n",
    "\n",
    "log_dir = f'runs/{folder}/{run_name}_{datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")}'\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "num_epochs=15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup transform and define CustomDataset class for iterable datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Use the train.csv and val.csv csv files to create two datasets\n",
    "class CustomDataset(Dataset):\n",
    "    # csv_file is a file path, root_dir is the root of the dataset, transform is an optional instantiation of transforms.Compose\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    # How many items are in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Get a specific item from the dataset by index\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx,0] # column 0 contains file paths\n",
    "        img_path = f\"{self.root_dir}/{img_name}\"\n",
    "        # A basic image loader that retruns a Python Imaging Library (PIL) object\n",
    "        image = default_loader(img_path)\n",
    "        label = self.data.iloc[idx,1] # column 1 contains image class\n",
    "\n",
    "        # If there is a transform function, transform the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../datasets/bean-leaf-lesions/\"\n",
    "\n",
    "# CSV file paths\n",
    "train_csv = data_path + 'train.csv'\n",
    "val_csv = data_path + 'val.csv'\n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(csv_file=train_csv, root_dir=data_path, transform=transform)\n",
    "val_dataset = CustomDataset(csv_file=val_csv, root_dir=data_path, transform=transform)\n",
    "\n",
    "# Create Dataloaders\n",
    "batch_size=32\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 33\n",
      "Val batches: 5\n"
     ]
    }
   ],
   "source": [
    "# print how many batches are in the train and val dataloaders\n",
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Val batches: {len(val_data_loader)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module is the base class for all NN modules in pytorch\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # CNN class inherits from nn.Module\n",
    "        # Inheriting allows the use of functions from the parent class\n",
    "        super(CNN, self).__init__()\n",
    "        # Input = RGB, Output = number of filters/kernels in the layer, \n",
    "        # kernel_size = height&width of convolutional window, padding = number of 0 pixels added to edge of image\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels= 16, kernel_size= 3, padding=1)\n",
    "        # kernel_size = size of the max pooling window\n",
    "        # step_size determines how much the kernel moves by after each pooling operation\n",
    "        # With both of these being 2, the spatial dimension is halved\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # The number of outputs increases with the depth of the model because the complexity of the\n",
    "        # learned representations requires more neurons\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
    "        # 32 is the previous layers output size, and 8 is the size of the image after the pooling layer\n",
    "        # so 32 * 8 * 8 is the number of dimensions in the flattened tensor\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 3) # 3 possible classes in the dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply conv1 to the input, apply relu activation, apply max pooling\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        # x.view is a pytorch fn used to reshape tensors\n",
    "        # This flattens the tensor into a 1D vector of 32*8*8 elements\n",
    "        # The -1 is a placeholder to allow pytorch to automatically compute the size of that dimension\n",
    "        # The total number of elements in the tensor is used (e.g. tensor contains 100 elems, x.view(5, -1) would resolve -1 to 20)\n",
    "        x = x.view(-1, 32 * 32 * 32)\n",
    "        # Puth the flattened tensor through a fully connected layer\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        # Resolve to one of the 3 possible classes\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # Computes softmax as well as loss\n",
    "\n",
    "# Momentum at 0.9 means that the gradient will be 90% influenced by previous gradients and 10% by the current\n",
    "# Allows for faster convergence by moving more in steeper situations and less in shallow\n",
    "optimiser = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0694806503527092, Training Accuracy: 0.42649903288201163, Training F1 Score: 0.4248355943542979\n",
      "Epoch 2, Loss: 0.9725504156314966, Training Accuracy: 0.5512572533849129, Training F1 Score: 0.548715985856119\n",
      "Epoch 3, Loss: 0.8445745309193929, Training Accuracy: 0.6353965183752418, Training F1 Score: 0.634138632992825\n",
      "Epoch 4, Loss: 0.7490016944480665, Training Accuracy: 0.6847195357833655, Training F1 Score: 0.6828214958092552\n",
      "Epoch 5, Loss: 0.6847857345234264, Training Accuracy: 0.7059961315280464, Training F1 Score: 0.7026922078169662\n",
      "Epoch 6, Loss: 0.6304027718124967, Training Accuracy: 0.7398452611218569, Training F1 Score: 0.7384552511579551\n",
      "Epoch 7, Loss: 0.6277282373471693, Training Accuracy: 0.7282398452611218, Training F1 Score: 0.725942990656967\n",
      "Epoch 8, Loss: 0.6032537845048037, Training Accuracy: 0.7504835589941973, Training F1 Score: 0.7482576810433499\n",
      "Epoch 9, Loss: 0.5874897864731875, Training Accuracy: 0.7572533849129593, Training F1 Score: 0.7570333599240685\n",
      "Epoch 10, Loss: 0.5366531881419095, Training Accuracy: 0.781431334622824, Training F1 Score: 0.7802560220829194\n",
      "Epoch 11, Loss: 0.5095485918449633, Training Accuracy: 0.7911025145067698, Training F1 Score: 0.7908153414005104\n",
      "Epoch 12, Loss: 0.4845530101747224, Training Accuracy: 0.8017408123791102, Training F1 Score: 0.8004644761536076\n",
      "Epoch 13, Loss: 0.4712511815808036, Training Accuracy: 0.8075435203094777, Training F1 Score: 0.8063682833174358\n",
      "Epoch 14, Loss: 0.4643821869835709, Training Accuracy: 0.8075435203094777, Training F1 Score: 0.8070693818629557\n",
      "Epoch 15, Loss: 0.4484513555512284, Training Accuracy: 0.816247582205029, Training F1 Score: 0.8155268143961015\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss=0.0\n",
    "    total_predictions = []\n",
    "    true_labels = []\n",
    "    # Generates tuples containing index 'i' and elements 'data'\n",
    "    # the 0 argument sets the index to begin at 0\n",
    "    # the index refers to which batch of data we are currently looking at\n",
    "    # the data refers to the collection of data points in that batch, with inputs and labels\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # clears any previously generated gradients\n",
    "        # For each data point, we n\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # computes gradients of the loss wrt model params with back-prop\n",
    "        loss.backward()\n",
    "        # Updates model parameters using gradients from loss.backward(), which stored the gradients within the model params\n",
    "        # Since the model params are stored in the optimiser anyway, we do no need to explicitly handle this arg\n",
    "        optimiser.step()\n",
    "\n",
    "        # Record predictions and labels for computing metrics\n",
    "        # outputs, 1 means that we record the maximum value along the 1st dimension of the outputs tensor\n",
    "        # the _ is because it returns the value of the item as well as the index, and we do not need the value \n",
    "        # (we only need the predicted class)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        # we convert predicted to a list for consistency reasons. \n",
    "        # It is not strictly necessary in this case but is good practice\n",
    "        total_predictions.extend(predicted.tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # calculate accuracy and F1 score after each epoch\n",
    "    train_accuracy = accuracy_score(true_labels, total_predictions)\n",
    "    # type of average can be chosen as appropriate\n",
    "    # With macro, each class contributes equally to the final metric regardless of class imbalance\n",
    "    # F1 score is computed for each class independently and then the average is taken\n",
    "    # This choice might not be appropriate in imbalanced datasets\n",
    "    train_f1_score = f1_score(true_labels, total_predictions, average='macro')\n",
    "\n",
    "    # Log metrics using SummaryWriter\n",
    "    writer.add_scalar('Loss/train', running_loss / len(train_dataloader), epoch+1)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy, epoch+1)\n",
    "    writer.add_scalar('F1_Score/train', train_f1_score, epoch+1)\n",
    "\n",
    "    # Print the metrics after each epoch\n",
    "    # Loss is running_loss/len(train_dataloader) because the running loss is the sum of the loss over all the batches\n",
    "    # Dividing it by the number of batches gives us the average loss\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_dataloader)}, Training Accuracy: {train_accuracy}, Training F1 Score: {train_f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 6401), started 2:24:25 ago. (Use '!kill 6401' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6828ac79dd7ef872\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6828ac79dd7ef872\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ffb79dba2b0659ac21bde8e88d8bd6113f5558632155a003a95479e7df48b66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
